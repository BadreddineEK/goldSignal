"""
06_benchmark.py â€” Tableau de bord de comparaison des modÃ¨les (vitrine ML).

Lit les rÃ©sultats entraÃ®nÃ©s en page 03 (st.session_state) et affiche :
  1. Tableau comparatif multi-mÃ©triques (DA%, Brier, Log-Loss, RMSE)
  2. Radar chart multi-critÃ¨res (vue synthÃ©tique)
  3. Courbes d'apprentissage LSTM (convergence train/val par fold)
  4. Heatmap des poids d'attention LSTM (interprÃ©tabilitÃ©)
  5. Matrices de confusion normalisÃ©es (tous modÃ¨les)
  6. Directional Accuracy par fold â€” stabilitÃ© temporelle
  7. Courbes de calibration (fiabilitÃ© probabiliste)
  8. Test de Diebold-Mariano â€” significativitÃ© statistique
  9. Poids du mÃ©ta-apprenant hybride (contribution de chaque modÃ¨le)
"""

import logging
import numpy as np
import pandas as pd
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import streamlit as st

logger = logging.getLogger(__name__)

st.header("ğŸ“ Benchmark des ModÃ¨les ML")
st.caption(
    "Ã‰valuation rigoureuse en walk-forward â€” tous les rÃ©sultats proviennent "
    "d'un protocole zÃ©ro-leakage. Lancez d'abord l'entraÃ®nement sur la page **PrÃ©dictions ML**."
)

# ---------------------------------------------------------------------------
# VÃ©rification des donnÃ©es disponibles
# ---------------------------------------------------------------------------
KEYS = ["ml_results", "arima_result", "lstm_result", "hybrid_result", "rw_metrics"]
available = {k: st.session_state.get(k) for k in KEYS}

if not available["ml_results"]:
    st.warning("âš ï¸ Aucun modÃ¨le entraÃ®nÃ©. Rendez-vous sur **ğŸ¤– PrÃ©dictions ML** et lancez l'entraÃ®nement.")
    st.stop()

ml   = available["ml_results"]
arima = available["arima_result"] or {}
lstm  = available["lstm_result"] or {}
hyb   = available["hybrid_result"] or {}
rw    = available["rw_metrics"] or {}
horizon = st.session_state.get("horizon", 5)

# Dictionnaire unifiÃ© des modÃ¨les disponibles
models: dict[str, dict] = {}
if rw:
    models["Random Walk"] = {"metrics": rw, "color": "#64748b", "symbol": "â—"}
if arima.get("metrics"):
    models["ARIMA"] = {**arima, "color": "#8b5cf6", "symbol": "â—†"}
if ml.get("rf", {}).get("metrics"):
    models["RandomForest"] = {**ml["rf"], "color": "#f59e0b", "symbol": "â–²"}
if ml.get("xgb") and ml["xgb"].get("metrics"):
    models["XGBoost"] = {**ml["xgb"], "color": "#ef4444", "symbol": "â– "}
if lstm.get("metrics"):
    models["LSTM"] = {**lstm, "color": "#3b82f6", "symbol": "â—‰"}
if hyb.get("metrics"):
    models["Stacking"] = {**hyb, "color": "#22c55e", "symbol": "â˜…"}

try:
    from models.lstm_model import diebold_mariano_test
    from models.hybrid_model import brier_score_multiclass, log_loss_multiclass
    from sklearn.preprocessing import LabelEncoder
    SCORING_OK = True
except ImportError:
    SCORING_OK = False


# ---------------------------------------------------------------------------
# â”€â”€ 1. Tableau comparatif multi-mÃ©triques â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ---------------------------------------------------------------------------
st.subheader(f"ğŸ“Š Comparaison complÃ¨te â€” Horizon {horizon}j")

rows = []
le = LabelEncoder(); le.fit([-1, 0, 1])

for name, res in models.items():
    m = res.get("metrics", {})
    row = {
        "ModÃ¨le":        name,
        "DA (%)":        m.get("da_pct", np.nan),
        "Folds (moy DA)":np.mean(m.get("da_folds", [m.get("da_pct", np.nan)])),
        "RMSE":          m.get("rmse", np.nan),
        "MAE":           m.get("mae",  np.nan),
        "N Ã©chantillons":int(m.get("n_samples", 0)),
        "Brier â†“":       np.nan,
        "Log-Loss â†“":    np.nan,
    }
    # Brier + Log-Loss si probas disponibles
    if SCORING_OK and "probabilities" in res and "actuals" in res:
        try:
            probas   = np.array(res["probabilities"])
            actuals  = res["actuals"].values
            y_enc    = le.transform(actuals[:len(probas)])
            row["Brier â†“"]    = round(brier_score_multiclass(y_enc, probas[:len(y_enc)]), 4)
            row["Log-Loss â†“"] = round(log_loss_multiclass(y_enc, probas[:len(y_enc)]), 4)
        except Exception:
            pass
    rows.append(row)

df_bench = pd.DataFrame(rows)

def color_da(val):
    if pd.isna(val): return ""
    if val >= 57: return "background-color:#14532d; color:#4ade80; font-weight:800"
    if val >= 54: return "background-color:#166534; color:#86efac; font-weight:700"
    if val >= 51: return "color:#f59e0b"
    return "color:#ef4444"

def color_brier(val):
    if pd.isna(val): return ""
    if val <= 0.55: return "color:#22c55e; font-weight:700"
    if val <= 0.65: return "color:#f59e0b"
    return "color:#ef4444"

st.dataframe(
    df_bench.style
        .applymap(color_da, subset=["DA (%)", "Folds (moy DA)"])
        .applymap(color_brier, subset=["Brier â†“"])
        .format({
            "DA (%)": "{:.1f}",
            "Folds (moy DA)": "{:.1f}",
            "RMSE": "{:.5f}",
            "MAE":  "{:.5f}",
            "Brier â†“": "{:.4f}",
            "Log-Loss â†“": "{:.4f}",
        }, na_rep="â€”"),
    width="stretch",
    hide_index=True,
)

st.caption("""
**Lecture** : DA% > 55% = signal Ã©conomiquement utile | Brier â†“ = meilleure calibration probabiliste |
Log-Loss â†“ = meilleure incertitude prÃ©dictive | toutes mÃ©triques calculÃ©es sur donnÃ©es **hors-Ã©chantillon**.
""")

st.markdown("---")

# ---------------------------------------------------------------------------
# â”€â”€ 2. Radar chart multi-critÃ¨res â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ---------------------------------------------------------------------------
st.subheader("ğŸ•¸ï¸ Vue Radar â€” Profil de Chaque ModÃ¨le")

radar_axes = ["DA (%)", "StabilitÃ©\nfolds", "Calibration\n(1-Brier)", "RapiditÃ©\n(proxy)", "InterprÃ©t."]

interp_scores = {
    "Random Walk": 5, "ARIMA": 4, "RandomForest": 4, "XGBoost": 3, "LSTM": 2, "Stacking": 2
}
speed_scores = {
    "Random Walk": 5, "ARIMA": 4, "RandomForest": 3, "XGBoost": 3, "LSTM": 2, "Stacking": 1
}

fig_radar = go.Figure()

def _normalize(val, lo, hi):
    return max(0, min(5, (val - lo) / max(hi - lo, 1e-6) * 5))

for name, res in models.items():
    m = res.get("metrics", {})
    da      = m.get("da_pct", 50)
    folds   = m.get("da_folds", [da])
    stab    = 5 - _normalize(np.std(folds), 0, 15)   # stabilitÃ© inversÃ©e
    brier   = res.get("metrics", {}).get("brier", np.nan) if "brier" in m else np.nan
    calib   = (1 - float(brier)) * 5 if not np.isnan(brier) else 3.0
    speed   = speed_scores.get(name, 3)
    interp  = interp_scores.get(name, 3)
    da_norm = _normalize(da, 45, 70)

    vals = [da_norm, stab, calib, speed, interp]
    vals += [vals[0]]  # fermer le polygone

    fig_radar.add_trace(go.Scatterpolar(
        r=vals,
        theta=radar_axes + [radar_axes[0]],
        fill="toself",
        name=name,
        line=dict(color=res["color"], width=2),
        opacity=0.65,
    ))

fig_radar.update_layout(
    polar=dict(radialaxis=dict(visible=True, range=[0, 5])),
    height=400,
    paper_bgcolor="rgba(0,0,0,0)",
    plot_bgcolor="rgba(15,23,42,0.8)",
    legend=dict(orientation="h", y=-0.1),
    margin=dict(l=20, r=20, t=20, b=40),
)
st.plotly_chart(fig_radar, width="stretch")
st.markdown("---")

# ---------------------------------------------------------------------------
# â”€â”€ 3. Courbes d'apprentissage LSTM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ---------------------------------------------------------------------------
if lstm.get("learning_curves"):
    st.subheader("ğŸ“‰ Convergence LSTM â€” Train vs Validation Loss")
    curves = lstm["learning_curves"]

    fig_lc = make_subplots(
        rows=1, cols=len(curves),
        subplot_titles=[f"Fold {i+1}" for i in range(len(curves))],
        shared_yaxes=True,
    )
    for i, c in enumerate(curves):
        n = len(c.get("train", []))
        ep = list(range(n))
        fig_lc.add_trace(go.Scatter(
            x=ep, y=c["train"], mode="lines", name="Train" if i == 0 else None,
            line=dict(color="#3b82f6", width=2),
            showlegend=(i == 0),
        ), row=1, col=i + 1)
        fig_lc.add_trace(go.Scatter(
            x=list(range(len(c.get("val", [])))), y=c.get("val", []),
            mode="lines", name="Validation" if i == 0 else None,
            line=dict(color="#f59e0b", width=2, dash="dot"),
            showlegend=(i == 0),
        ), row=1, col=i + 1)

    fig_lc.update_layout(
        height=280,
        paper_bgcolor="rgba(0,0,0,0)",
        plot_bgcolor="rgba(15,23,42,0.8)",
        legend=dict(orientation="h", y=1.08),
        margin=dict(l=0, r=0, t=40, b=0),
        font=dict(color="#e2e8f0"),
    )
    fig_lc.update_yaxes(gridcolor="#1e293b")
    st.plotly_chart(fig_lc, width="stretch")
    st.caption(
        "Early stopping actif : l'entraÃ®nement s'arrÃªte quand la val-loss stagne "
        "(patience configurable). Le gap train/val mesure le surapprentissage rÃ©siduel."
    )
    st.markdown("---")

# ---------------------------------------------------------------------------
# â”€â”€ 4. Heatmap attention LSTM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ---------------------------------------------------------------------------
if lstm.get("attention") and len(lstm["attention"]) > 0:
    st.subheader("ğŸ” Attention LSTM â€” Quels Timesteps sont Informatifs ?")

    try:
        # Moyenne des poids d'attention sur tous les folds + toutes les prÃ©dictions
        all_attn = np.vstack([a for fold in lstm["attention"] for a in fold])
        avg_attn = all_attn.mean(axis=0)     # (seq_len,)
        seq_len  = len(avg_attn)

        fig_attn = go.Figure(go.Bar(
            x=list(range(-seq_len + 1, 1)),
            y=avg_attn[::-1] if len(avg_attn) > 0 else avg_attn,
            marker=dict(
                color=avg_attn[::-1],
                colorscale="YlOrRd",
                showscale=True,
                colorbar=dict(title="Poids"),
            ),
        ))
        fig_attn.update_layout(
            xaxis_title="Lag (jours avant la prÃ©diction)",
            yaxis_title="Poids d'attention moyen",
            height=250,
            paper_bgcolor="rgba(0,0,0,0)",
            plot_bgcolor="rgba(15,23,42,0.8)",
            yaxis=dict(gridcolor="#1e293b"),
            margin=dict(l=0, r=0, t=10, b=0),
        )
        st.plotly_chart(fig_attn, width="stretch")
        peak_lag = -seq_len + 1 + int(np.argmax(avg_attn[::-1]))
        st.caption(
            f"Le modÃ¨le accorde le plus d'attention au lag **{abs(peak_lag)}j** avant la prÃ©diction. "
            "Les pics d'attention correspondent gÃ©nÃ©ralement aux pÃ©riodes de forte volatilitÃ© rÃ©cente."
        )
    except Exception as e:
        logger.debug("Attention heatmap : %s", e)

    st.markdown("---")

# ---------------------------------------------------------------------------
# â”€â”€ 5. Matrices de confusion normalisÃ©es â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ---------------------------------------------------------------------------
models_with_preds = {k: v for k, v in models.items()
                     if "predictions" in v and "actuals" in v
                     and k not in ("Random Walk",)}

if models_with_preds:
    st.subheader("ğŸ² Matrices de Confusion NormalisÃ©es")
    le2 = LabelEncoder(); le2.fit([-1, 0, 1])
    labels_str = ["Baissier (-1)", "Neutre (0)", "Haussier (+1)"]

    ncols = min(3, len(models_with_preds))
    nrows = (len(models_with_preds) + ncols - 1) // ncols
    names_list = list(models_with_preds.keys())

    fig_cm = make_subplots(
        rows=nrows, cols=ncols,
        subplot_titles=names_list,
        vertical_spacing=0.12, horizontal_spacing=0.08,
    )

    for idx, (name, res) in enumerate(models_with_preds.items()):
        row, col = divmod(idx, ncols)
        try:
            preds   = res["predictions"].values
            actuals = res["actuals"].values
            from sklearn.metrics import confusion_matrix
            cm = confusion_matrix(actuals, preds, labels=[-1, 0, 1], normalize="true")

            fig_cm.add_trace(go.Heatmap(
                z=cm,
                x=["â†“ Bas", "â†’ Neutre", "â†‘ Haut"],
                y=["Bas", "Neutre", "Haut"],
                colorscale="Blues",
                showscale=False,
                zmin=0, zmax=1,
                text=[[f"{v:.0%}" for v in row_] for row_ in cm],
                texttemplate="%{text}",
            ), row=row + 1, col=col + 1)
        except Exception:
            pass

    fig_cm.update_layout(
        height=300 * nrows,
        paper_bgcolor="rgba(0,0,0,0)",
        font=dict(color="#e2e8f0"),
        margin=dict(l=0, r=0, t=60, b=0),
    )
    st.plotly_chart(fig_cm, width="stretch")
    st.caption(
        "Matrices normalisÃ©es par ligne (recall). La diagonale = taux de bonne classification. "
        "Un bon classifieur a une diagonale dominante."
    )
    st.markdown("---")

# ---------------------------------------------------------------------------
# â”€â”€ 6. DA% par fold â€” stabilitÃ© temporelle â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ---------------------------------------------------------------------------
st.subheader("ğŸ“ˆ StabilitÃ© Temporelle â€” DA% par Fold")

fig_folds = go.Figure()
max_folds = 0

for name, res in models.items():
    folds = res.get("metrics", {}).get("da_folds", [])
    if not folds:
        continue
    max_folds = max(max_folds, len(folds))
    fig_folds.add_trace(go.Scatter(
        x=[f"Fold {i+1}" for i in range(len(folds))],
        y=folds,
        mode="lines+markers+text",
        name=name,
        line=dict(color=res["color"], width=2),
        marker=dict(size=8),
        text=[f"{v:.0f}%" for v in folds],
        textposition="top center",
    ))

fig_folds.add_hline(y=50, line_dash="dash", line_color="#64748b", annotation_text="Random (50%)")
fig_folds.add_hline(y=55, line_dash="dot",  line_color="#22c55e",  annotation_text="Cible (55%)")
fig_folds.update_layout(
    height=320,
    yaxis=dict(range=[30, 80], title="DA (%)", gridcolor="#1e293b"),
    paper_bgcolor="rgba(0,0,0,0)",
    plot_bgcolor="rgba(15,23,42,0.8)",
    legend=dict(orientation="h", y=1.05),
    margin=dict(l=0, r=0, t=10, b=0),
)
st.plotly_chart(fig_folds, width="stretch")
st.caption(
    "Un modÃ¨le robuste maintient une DA% stable d'un fold Ã  l'autre. "
    "Une chute brutale sur les derniers folds signale une rupture de rÃ©gime de marchÃ©."
)
st.markdown("---")

# ---------------------------------------------------------------------------
# â”€â”€ 7. Courbes de calibration (fiabilitÃ© probabiliste) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ---------------------------------------------------------------------------
models_with_probas = {k: v for k, v in models.items()
                      if "probabilities" in v and "actuals" in v}

if models_with_probas:
    st.subheader("ğŸ¯ Calibration Probabiliste â€” FiabilitÃ© des Confiances")
    st.caption(
        "Un modÃ¨le calibrÃ© prÃ©dit p=70% exactement dans 70% des cas. "
        "La diagonale parfaite = calibration parfaite (modÃ¨le Brier-optimal)."
    )

    fig_cal = go.Figure()
    fig_cal.add_trace(go.Scatter(
        x=[0, 1], y=[0, 1], mode="lines",
        line=dict(color="#64748b", dash="dash"),
        name="Calibration parfaite",
    ))

    le3 = LabelEncoder(); le3.fit([-1, 0, 1])
    n_bins = 10
    bin_edges = np.linspace(0, 1, n_bins + 1)

    for name, res in models_with_probas.items():
        try:
            probas  = np.array(res["probabilities"])
            actuals = res["actuals"].values
            y_enc   = le3.transform(actuals[:len(probas)])
            # Classe haussier (idx 2)
            p_haussier = probas[:len(y_enc), 2]
            y_haussier = (y_enc == 2).astype(int)
            bin_frac_pos, bin_mean_pred = [], []
            for i in range(n_bins):
                mask = (p_haussier >= bin_edges[i]) & (p_haussier < bin_edges[i + 1])
                if mask.sum() >= 3:
                    bin_frac_pos.append(y_haussier[mask].mean())
                    bin_mean_pred.append(p_haussier[mask].mean())
            if bin_mean_pred:
                fig_cal.add_trace(go.Scatter(
                    x=bin_mean_pred, y=bin_frac_pos,
                    mode="lines+markers",
                    name=name,
                    line=dict(color=res["color"], width=2),
                    marker=dict(size=6),
                ))
        except Exception:
            pass

    fig_cal.update_layout(
        height=300,
        xaxis=dict(title="ProbabilitÃ© haussier prÃ©dite", range=[0, 1], gridcolor="#1e293b"),
        yaxis=dict(title="FrÃ©quence rÃ©elle haussier",    range=[0, 1], gridcolor="#1e293b"),
        paper_bgcolor="rgba(0,0,0,0)",
        plot_bgcolor="rgba(15,23,42,0.8)",
        legend=dict(orientation="h", y=1.05),
        margin=dict(l=0, r=0, t=10, b=0),
    )
    st.plotly_chart(fig_cal, width="stretch")
    st.markdown("---")

# ---------------------------------------------------------------------------
# â”€â”€ 8. Test de Diebold-Mariano (significativitÃ© statistique) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ---------------------------------------------------------------------------
if SCORING_OK:
    models_dm = {k: v for k, v in models.items()
                 if "predictions" in v and "actuals" in v and k != "Random Walk"}

    if len(models_dm) >= 2:
        st.subheader("ğŸ“ Test Diebold-Mariano â€” SignificativitÃ© Statistique")
        st.caption(
            "Hâ‚€ : les deux modÃ¨les sont Ã©quivalents. p < 0.05 â†’ diffÃ©rence significative. "
            "MÃ©thode : DM avec correction Harvey-Newbold-Leybourne (1997)."
        )

        dm_matrix = pd.DataFrame(index=list(models_dm.keys()), columns=list(models_dm.keys()), dtype=object)

        for n1, r1 in models_dm.items():
            for n2, r2 in models_dm.items():
                if n1 == n2:
                    dm_matrix.loc[n1, n2] = "â€”"
                    continue
                try:
                    common = r1["actuals"].index.intersection(r2["actuals"].index)
                    e1 = np.abs(r1["predictions"].loc[common].values - r1["actuals"].loc[common].values)
                    e2 = np.abs(r2["predictions"].loc[common].values - r2["actuals"].loc[common].values)
                    dm = diebold_mariano_test(e1, e2)
                    p = dm["p_value"]
                    flag = "âœ…" if p < 0.05 else "Â·"
                    dm_matrix.loc[n1, n2] = f"p={p:.3f} {flag}"
                except Exception:
                    dm_matrix.loc[n1, n2] = "n/d"

        st.dataframe(dm_matrix, width="stretch")
        st.caption("âœ… = significatif au seuil 5% | Â· = non-significatif")
        st.markdown("---")

# ---------------------------------------------------------------------------
# â”€â”€ 9. Poids du mÃ©ta-apprenant hybride â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ---------------------------------------------------------------------------
if hyb.get("meta_coefs") is not None and len(hyb.get("meta_coefs", [])) > 0:
    st.subheader("ğŸ”— Contribution des ModÃ¨les de Base (MÃ©ta-Apprenant)")
    st.caption(
        "Coefficients du mÃ©ta-apprenant LogisticRegression appris sur les probabilitÃ©s OOF. "
        "Un coefficient Ã©levÃ© = le modÃ¨le de base est informatif pour le mÃ©ta-classifieur."
    )
    try:
        coefs = np.array(hyb["meta_coefs"])
        if coefs.ndim == 2:
            # coefs shape: (3_classes, n_features=n_models*3)
            n_models_meta = coefs.shape[1] // 3
            model_names_meta = ["RF", "XGBoost", "LSTM"][:n_models_meta]
            class_names = ["Baissier", "Neutre", "Haussier"]

            for ci, cname in enumerate(class_names):
                if ci < coefs.shape[0]:
                    c_vals = coefs[ci]
                    fig_w = go.Figure()
                    palette = ["#f59e0b", "#ef4444", "#3b82f6"]
                    for mi, mname in enumerate(model_names_meta):
                        start = mi * 3
                        fig_w.add_trace(go.Bar(
                            name=mname,
                            x=class_names,
                            y=c_vals[start:start + 3],
                            marker_color=palette[mi % len(palette)],
                        ))
                    if ci == 0:
                        fig_w.update_layout(
                            barmode="group", height=240,
                            title=dict(text="Coefficients mÃ©ta-apprenant par classe de sortie"),
                            paper_bgcolor="rgba(0,0,0,0)",
                            plot_bgcolor="rgba(15,23,42,0.8)",
                            yaxis=dict(gridcolor="#1e293b"),
                            legend=dict(orientation="h", y=1.1),
                            margin=dict(l=0, r=0, t=40, b=0),
                        )
                        st.plotly_chart(fig_w, width="stretch")
                        break
    except Exception as e:
        logger.debug("Poids mÃ©ta-apprenant : %s", e)

    st.markdown("---")

# ---------------------------------------------------------------------------
# Note mÃ©thodologique
# ---------------------------------------------------------------------------
with st.expander("ğŸ“š Note mÃ©thodologique"):
    st.markdown(f"""
    ### Protocole de validation â€” GoldSignal V2

    **DonnÃ©es** : XAU/EUR (log-rendements) â€” source yfinance (GC=F + EURUSD=X)

    **Cible** : classification ternaire {{baissier, neutre, haussier}} Ã  horizon **{horizon} jours**,
    seuil = Â±{st.session_state.get('seuil_direction', 0.3):.1f}% de log-rendement cumulÃ©.

    **Protocole** : Walk-forward expanding window â€” aucune information future ne filtre vers le passÃ©.
    Normalisation MinMaxScaler ajustÃ©e sur le train de chaque fold uniquement.

    **MÃ©triques** :
    - **DA%** (Directional Accuracy) : proportion de directions correctes
    - **Brier Score** (Zadrozny & Elkan 2002) : proper scoring rule, mesure la qualitÃ© des probabilitÃ©s
    - **Log-Loss** : proper scoring rule logarithmique (pÃ©nalise les confiances erronÃ©es)
    - **Test DM** (Diebold-Mariano 1995, corr. Harvey-Newbold-Leybourne 1997)

    **ModÃ¨les** :
    | ModÃ¨le | Classe | ParamÃ©trage |
    |--------|--------|-------------|
    | Random Walk | Baseline | prÃ©diction = signe du dernier retour |
    | ARIMA | Classique | sÃ©lection ordre par AIC sur chaque fold train |
    | RandomForest | Ensemble | class_weight=balanced, min_samples_leaf=10 |
    | XGBoost | Gradient Boosting | multi:softprob, early_stopping=50 |
    | LSTM | Deep Learning | bi-LSTM + dot-product attention + early stopping |
    | Stacking | MÃ©ta-apprentissage | LogReg sur probas OOF (Wolpert 1992) |

    **RÃ©fÃ©rence** : Wolpert (1992) *Stacked Generalization*. Neural Networks 5(2).
    Diebold & Mariano (1995) *Comparing Predictive Accuracy*. JBES 13(3).
    """)
